{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb32d211-8429-4fe1-b0a1-f12131e478ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers\n",
    "# !pip install datasets --upgrade \n",
    "# installations\n",
    "# !pip install matplotlib\n",
    "# !pip install pillow\n",
    "# !pip install tifffile\n",
    "# !pip install datasets\n",
    "# (otherwise issues to load the bean dataset)\n",
    "# !pip install datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19487d13-2f13-48f0-bbf3-970748deb300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once, unless you have any update for the data structure\n",
    "# create files with 47 channels at folder csiseminar/project/dataset/47_channels/\n",
    "# the output is relevant to check that the files are not only zeros (viewing them, it looks like \n",
    "# it is only grey\n",
    "# %run preprocess_tif_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529bb5ee-8ea5-4955-a341-adff3df49a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eilaarich-landkof/miniconda3/envs/seminar/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils.dummy_vision_objects import ImageGPTFeatureExtractor\n",
    "import random\n",
    "from PIL import ImageDraw, ImageFont, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b1a407-2ee4-4b0e-aee7-6662cc759320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8402e5-9441-49dc-9bbc-b2bcc0d5f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_to_non_progressors_images = os.path.join(os.getcwd(),\"dataset/47_channels/labels/nonprogressor/\")\n",
    "path_to_progressors_images = os.path.join(os.getcwd(),\"dataset/47_channels/labels/progressor/\")\n",
    "CHANNELS = 47\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56a572a-5d2b-49e4-970d-3e848935f9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/nonprogressor/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_non_progressors_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afdf940f-6410-4e0a-a823-0364fa40332b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/progressor/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_progressors_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c6c991-3dc4-473f-8399-0617b83d2855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203.jpg 2304.tif 2320.jpg 2328.tif 3110.jpg 3117.tif 3129.jpg 5304.tif\n",
      "2203.tif 2309.jpg 2320.tif 2331.jpg 3110.tif 3119.jpg 3129.tif 5305.jpg\n",
      "2204.jpg 2309.tif 2321.jpg 2331.tif 3111.jpg 3119.tif 4410.jpg 5305.tif\n",
      "2204.tif 2311.jpg 2321.tif 3102.jpg 3111.tif 3120.jpg 4410.tif 5306.jpg\n",
      "2206.jpg 2311.tif 2325.jpg 3102.tif 3112.jpg 3120.tif 4411.jpg 5306.tif\n",
      "2206.tif 2313.jpg 2325.tif 3104.jpg 3112.tif 3121.jpg 4411.tif 5308.jpg\n",
      "2302.jpg 2313.tif 2326.jpg 3104.tif 3114.jpg 3121.tif 5202.jpg 5308.tif\n",
      "2302.tif 2318.jpg 2326.tif 3107.jpg 3114.tif 3125.jpg 5202.tif 5309.jpg\n",
      "2303.jpg 2318.tif 2327.jpg 3107.tif 3115.jpg 3125.tif 5302.jpg 5309.tif\n",
      "2303.tif 2319.jpg 2327.tif 3108.jpg 3115.tif 3128.jpg 5302.tif 6206.jpg\n",
      "2304.jpg 2319.tif 2328.jpg 3108.tif 3117.jpg 3128.tif 5304.jpg 6206.tif\n"
     ]
    }
   ],
   "source": [
    "!ls $path_to_non_progressors_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b7bd2b-a22b-48fc-8e0e-9db2bf563cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_progressors_images_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "400f877d-acfc-48bc-9061-3d8ed9ca7515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 33\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 71\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 47\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 45\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 103\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 93\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 57\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 105\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 122\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 51\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 190\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 109\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 232\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 223\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 109\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 186\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 60\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 155\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 79\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 148\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 46\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 25\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 23\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 90\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 83\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 233\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 25\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 40\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 29\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 107\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 82\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 36\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 68\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 42\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 90\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 104\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 77\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 38\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 45\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 193\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 37\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 23\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 36\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 62\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 47\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 154\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 49\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 77\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 39\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 86\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 79\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 32\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 52\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 232\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 232\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 57\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 59\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n",
      "new_image shape: (3, 4053, 4053)\n",
      "new_image dtype: uint8\n",
      "Min value: 0\n",
      "Max value: 224\n",
      "new_image_pil dtype: <class 'PIL.Image.Image'> \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%run prepare_dataset_from_image_files.ipynb\n",
    "non_progressors_images_list = os.listdir(path_to_non_progressors_images)\n",
    "progressors_images_list = os.listdir(path_to_progressors_images)\n",
    "non_progressors_images_list = [os.path.join(path_to_non_progressors_images,x) for x in non_progressors_images_list if \".tif\"in x]\n",
    "progressors_images_list = [os.path.join(path_to_progressors_images,x) for x in progressors_images_list if \".tif\" in x]\n",
    "\n",
    "\n",
    "non_progressors_data_list = load_47_channel_tif_to_dataset_object(non_progressors_images_list,\"nonprogressor\")\n",
    "progressors_data_list = load_47_channel_tif_to_dataset_object(progressors_images_list,\"progressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1125dcad-5686-44ab-83e5-e280e59fdf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image': <PIL.Image.Image image mode=RGB size=4053x3>,\n",
       "  'image_file_path': '/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/nonprogressor/2206.jpg',\n",
       "  'labels': 0},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=4053x3>,\n",
       "  'image_file_path': '/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/nonprogressor/3119.jpg',\n",
       "  'labels': 0},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=4053x3>,\n",
       "  'image_file_path': '/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/nonprogressor/3125.jpg',\n",
       "  'labels': 0},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=4053x3>,\n",
       "  'image_file_path': '/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/nonprogressor/2204.jpg',\n",
       "  'labels': 0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_progressors_data_list[0:4]\n",
    "# progressors_data_list = load_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0202cd6-a087-4ee8-a744-4db8a027e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hugging Face dataset directly from the data_list\n",
    "# %run prepare_dataset_from_image_files.ipynb\n",
    "# from dataset import dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_non_progressors = Dataset.from_dict({\n",
    "        \"image\": [item[\"image\"] for item in non_progressors_data_list],\n",
    "        \"image_file_path\": [item[\"image_file_path\"] for item in non_progressors_data_list],\n",
    "        \"labels\": [item[\"labels\"] for item in non_progressors_data_list],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6866f3b3-2387-4a76-a263-7d5206b75caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image': <PIL.Image.Image image mode=RGB size=4053x3>,\n",
       "  'image_file_path': '/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/progressor/6103.jpg',\n",
       "  'labels': 1},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=4053x3>,\n",
       "  'image_file_path': '/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/progressor/6102.jpg',\n",
       "  'labels': 1},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=4053x3>,\n",
       "  'image_file_path': '/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/progressor/3123.jpg',\n",
       "  'labels': 1},\n",
       " {'image': <PIL.Image.Image image mode=RGB size=4053x3>,\n",
       "  'image_file_path': '/Users/eilaarich-landkof/Documents/Code_Stanford/csiseminar/project/dataset/47_channels/labels/progressor/6201.jpg',\n",
       "  'labels': 1}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progressors_data_list[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c61f8fc8-ce68-4711-8624-189fbdb1bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hugging Face dataset directly from the data_list\n",
    "dataset_progressors = Dataset.from_dict({\n",
    "        \"image\": [item[\"image\"] for item in progressors_data_list],\n",
    "        \"image_file_path\": [item[\"image_file_path\"] for item in progressors_data_list],\n",
    "        \"labels\": [item[\"labels\"] for item in progressors_data_list],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e054908d-860e-49af-8a8d-a2c2e1cf42a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_file_path', 'labels'],\n",
       "    num_rows: 14\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_progressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcd935a1-fa6b-42e2-89af-57f9dfd39905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_file_path', 'labels'],\n",
       "    num_rows: 44\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_non_progressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6d9ab86-25a5-4a58-9277-3d9ddc6e6087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_non_progressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40b785ba-8b2a-4ba2-bfef-b36347dbb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "# Concatenate datasets\n",
    "ds = concatenate_datasets([dataset_non_progressors, dataset_progressors])\n",
    "\n",
    "# Split the combined dataset into train, validation, and test\n",
    "train_percentage = 0.8\n",
    "validation_percentage = 0.1\n",
    "test_percentage = 0.1\n",
    "\n",
    "train_size = int(len(ds) * train_percentage)\n",
    "validation_size = int(len(ds) * validation_percentage)\n",
    "test_size = len(ds) - train_size - validation_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7d0fd7f-8437-44bd-8a21-c24f2f144b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in train_dataset: 46\n",
      "Number of examples in validation_dataset: 5\n",
      "Number of examples in test_dataset: 7\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ds.select(range(train_size))\n",
    "validation_dataset = ds.select(range(train_size, train_size + validation_size))\n",
    "test_dataset = ds.select(range(train_size + validation_size, len(ds)))\n",
    "\n",
    "print(\"Number of examples in train_dataset:\", len(train_dataset))\n",
    "print(\"Number of examples in validation_dataset:\", len(validation_dataset))\n",
    "print(\"Number of examples in test_dataset:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cc40566-923f-4c70-90f2-5196f2cfd46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43faa354-8729-48a1-9dc6-705243ba746d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_file_path', 'labels'],\n",
       "    num_rows: 46\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b480bc6-22d0-46c9-9ef1-79801097c52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAD9UAAAADCAIAAAAYvSkgAAAAOklEQVR4nO3BMQEAAADCoPVPbQsvoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeBiOgAABl8s0ZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=4053x3>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = train_dataset['image'][0]\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2d0f362-1da1-401c-9666-ffe3ce233232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = ex['image']\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20515082-30b6-4126-a147-34a6e12bf8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_dataset['labels'][0]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19821510-e8be-4a6c-a1b8-37a8d4db267b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nonprogressor'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_string = class_label.int2str(labels)\n",
    "label_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3187afd-5931-4399-8877-ea736be96baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.ste(train_dataset['labels'][0])\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aef527b9-91fb-46e0-8f95-950149b44f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 43.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_validation_dataset = validation_dataset.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a158d4-6cc2-4acf-96ca-359799b8ded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eilaarich-landkof/miniconda3/envs/seminar/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 502/502 [00:00<00:00, 41.0kB/s]\n",
      "Downloading pytorch_model.bin:  12%|█▏        | 41.9M/346M [00:36<04:09, 1.22MB/s]"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Load the ViT model\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)  # Move the model to the desired device\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "# dataset = train_dataset\n",
    "preprocessed_train_dataset = train_dataset.map(preprocess_image)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(preprocessed_validation_dataset)\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12190cd7-61ff-4693-88f5-96c1f1feec6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ViTFeatureExtractor' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 21\u001b[0m\n\u001b[1;32m     10\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     11\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m---> 21\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device),  \u001b[38;5;66;03m# Move the model to the desired device\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     23\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mpreprocessed_train_dataset,\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ViTFeatureExtractor' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# # Loading the feature extractor \n",
    "# from transformers import ViTFeatureExtractor\n",
    "\n",
    "# model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "# model = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "# # also called feature_extractor\n",
    "\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     logging_dir='./logs',\n",
    "# )\n",
    "\n",
    "# from transformers import Trainer\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model.to(device),  # Move the model to the desired device\n",
    "#     args=training_args,\n",
    "#     train_dataset=preprocessed_train_dataset,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5aa9d-c24a-4f79-a6cf-340a13a9b8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d5199-a42f-4dea-8931-e90a0fe36017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93286f6e-480d-4226-be03-0cfdcdc80203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420bc7e5-a9db-43e1-a9e1-98c9ec89df24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b6dc0-4074-40ee-ab16-d49b7a47965a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25a79f-00bd-45ef-80f3-a008b4922355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f582c2-2c0a-4056-b4df-58cf0b1ff3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde210c4-4ba7-4658-bb40-1931a342c86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067cb864-4079-4e24-bc6d-71977ca71c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a6a1f-badc-4551-87ef-04205ac834a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf3920-2472-4b8b-8d79-8999b020b17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ebb0d0b-8021-46a6-8470-c1948f3442b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n",
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a2cacad-ce60-4166-8864-839a79b55029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_file_path', 'labels'],\n",
       "    num_rows: 58\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80d5b2d4-8ad3-41fc-a113-76584b3ffec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd45e832-b4d1-4088-9070-c5688ff4a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afa2acb4-a6b3-42bf-815c-ddc4b8124a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5680348-97c7-4ca2-b398-7e3e2a92604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46/46 [00:01<00:00, 35.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "%run prepare_dataset_from_image_files.ipynb\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# for GPU: conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch\n",
    "# for CPU: conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "preprocessed_train_dataset = train_dataset.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881744ee-4bcc-40b2-9f2d-3fb23fac8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# Assuming num_classes is the number of classes in your classification task\n",
    "num_classes = 2\n",
    "model = ViTForImageClassification.from_pretrained(model_name_or_path, num_labels=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1cd44-67e3-4e19-b806-0554c682b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using your training pipeline\n",
    "# ...\n",
    "\n",
    "# For example, using the Trainer class:\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
