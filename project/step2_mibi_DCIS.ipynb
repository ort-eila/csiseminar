{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ort-eila/csiseminar/blob/main/project/step2_mibi_DCIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grplo3KX8J_x",
        "outputId": "2b645045-66ad-430a-bb3d-0e32a65a2bf8"
      },
      "id": "grplo3KX8J_x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root_dir = \"/content/gdrive/MyDrive/Seminar/project/immune_310_project\"\n",
        "dataset_path=os.path.join(root_dir,\"dataset\",\"NPY_41_channels\",\"labels\")\n",
        "dataset_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "O_fxE6Lq8OCm",
        "outputId": "d349e812-f772-41be-8605-7debe06b6ca2"
      },
      "id": "O_fxE6Lq8OCm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "progressors_npy_dir = os.path.join(dataset_path,\"progressor\")\n",
        "print(\"progressors_npy_dir is {}\".format(progressors_npy_dir))\n",
        "non_progressors_npy_dir = os.path.join(dataset_path,\"nonprogressor\")\n",
        "print(\"non_progressors_npy_dir is {}\".format(non_progressors_npy_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShyAfwoc9K5J",
        "outputId": "09b99ac2-74c7-4f22-afcf-ce54b9f69a61"
      },
      "id": "ShyAfwoc9K5J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progressors_npy_dir is /content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/progressor\n",
            "non_progressors_npy_dir is /content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/nonprogressor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b628a431-95f8-4231-bb41-67a87df3e32d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b628a431-95f8-4231-bb41-67a87df3e32d",
        "outputId": "e9cc614e-dede-4195-dfa8-ca71b101bbf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/progressor/6103.npz', '/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/progressor/2306.npz', '/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/progressor/5310.npz', '/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/progressor/6102.npz', '/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/progressor/3105.npz']\n",
            "['/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/nonprogressor/5306.npz', '/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/nonprogressor/5308.npz', '/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/nonprogressor/2320.npz', '/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/nonprogressor/2318.npz', '/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/nonprogressor/2327.npz']\n"
          ]
        }
      ],
      "source": [
        "progressors_npy_files = os.listdir(progressors_npy_dir)\n",
        "non_progressors_npy_files = os.listdir(non_progressors_npy_dir)\n",
        "npy_n_channels_all = [os.path.join(progressors_npy_dir,x) for x in progressors_npy_files]\n",
        "npy_n_channels_all = npy_n_channels_all + [os.path.join(non_progressors_npy_dir,x) for x in non_progressors_npy_files]\n",
        "print(npy_n_channels_all[0:5])\n",
        "print(npy_n_channels_all[-5:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb32d211-8429-4fe1-b0a1-f12131e478ea",
      "metadata": {
        "id": "cb32d211-8429-4fe1-b0a1-f12131e478ea"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets transformers\n",
        "# !pip install datasets --upgrade\n",
        "# installations\n",
        "# !pip install matplotlib\n",
        "# !pip install pillow\n",
        "# !pip install tifffile\n",
        "# !pip install datasets\n",
        "# (otherwise issues to load the bean dataset)\n",
        "# !pip install datasets --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a25843ea-dce0-4e75-9673-39964c175e37",
      "metadata": {
        "id": "a25843ea-dce0-4e75-9673-39964c175e37"
      },
      "outputs": [],
      "source": [
        "# TODO: we need to pad the original data with two channels of zeros, so we will not loose data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f21a397-c45f-4970-b73b-52161f3aa1a2",
      "metadata": {
        "id": "8f21a397-c45f-4970-b73b-52161f3aa1a2"
      },
      "outputs": [],
      "source": [
        "# step 1: create a file of n channgels with all the relevant layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19487d13-2f13-48f0-bbf3-970748deb300",
      "metadata": {
        "id": "19487d13-2f13-48f0-bbf3-970748deb300"
      },
      "outputs": [],
      "source": [
        "# Run once, unless you have any update for the data structure\n",
        "# create files with 47 channels at folder csiseminar/project/dataset/47_channels/\n",
        "# the output is relevant to check that the files are not only zeros (viewing them, it looks like\n",
        "# it is only grey\n",
        "# %run preprocess_tif_data.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4faece17-d782-4001-ac57-2f37ab08c823",
      "metadata": {
        "id": "4faece17-d782-4001-ac57-2f37ab08c823"
      },
      "outputs": [],
      "source": [
        "# Step 2. create 224x224 images for the model and save them in a seperate foldet\n",
        "# with a folder name = dataset name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import re\n",
        "import os\n",
        "\n",
        "def create_rgb_images_from_channels(channel_data, output_shape, output_path, filename_prefix):\n",
        "    height, width = channel_data[0].shape\n",
        "    channels_per_rgb = 3\n",
        "\n",
        "\n",
        "    rgb_values = []\n",
        "    # for channel_index in range(0, len(channel_data)-1, channels_per_rgb):\n",
        "    #   print(\"channel_index is {}\".format(channel_index))\n",
        "    for col in range(0, width):\n",
        "      for row in range(0, height):\n",
        "        for channel_index in range(0, len(channel_data)-1, channels_per_rgb):\n",
        "          print(\"channel_index is {}\".format(channel_index))\n",
        "          print(\"row is {}\".format(row))\n",
        "          print(\"col is {}\".format(col))\n",
        "          R,G,B = 0,0,0 # prepare the padding\n",
        "          R = channel_data[channel_index][row, col]\n",
        "          if (channel_index+1 <= (len(channel_data)-1)):\n",
        "            G = channel_data[channel_index+1][row, col]\n",
        "          if (channel_index+2 <= (len(channel_data)-1)):\n",
        "            B = channel_data[channel_index+2][row, col]\n",
        "          rgb_val = (R,G,B)\n",
        "          print()\n",
        "          rgb_values.append(rgb_val)\n",
        "\n",
        "    print(\"rgb_values is {}\".format(rgb_values))\n",
        "    rgb_values_counter = 0\n",
        "    # Create the rgb_image using the accumulated rgb_values\n",
        "    num_of_images = len(rgb_values) // (output_shape[0]*output_shape[1]) + 1\n",
        "    # create at list one image\n",
        "    for img in range(num_of_images):\n",
        "      rgb_image = np.zeros((output_shape[0], output_shape[1], 3), dtype=np.uint8)\n",
        "      for img_i in range(output_shape[0]):\n",
        "        for img_j in range(output_shape[1]):\n",
        "          rgb_val = (0,0,0)\n",
        "          if rgb_values_counter < (len(rgb_values) - 1):\n",
        "            rgb_val = rgb_values[rgb_values_counter]\n",
        "            rgb_values_counter+=1\n",
        "            rgb_image[img_i,img_j,] = rgb_val\n",
        "            print(\"rgb_image[img_i,img_j] is {}\".format(rgb_image[img_i,img_j]))\n",
        "      pil_image = Image.fromarray(rgb_image)\n",
        "      file_name = f'{filename_prefix}_rgb_image_row{row}_col{col}.png'\n",
        "      pil_image.save(f'{output_path}/{file_name}')"
      ],
      "metadata": {
        "id": "8ApNZHLW9spz"
      },
      "id": "8ApNZHLW9spz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffb37f66-eb02-4466-a6e2-b2a413f4796b",
      "metadata": {
        "id": "ffb37f66-eb02-4466-a6e2-b2a413f4796b",
        "outputId": "6a1e9cc5-8505-4de1-9719-cef55b320a4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npz_file_path is /content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/NPY_41_channels/labels/progressor/6103.npz\n",
            "output_path is /content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_RGB_224_224/labels/progressor\n",
            "Keys in the NPZ file: ['channel_data']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Combine channel data\n",
        "# channel_data = [channel1, channel2, channel3, channel4, channel5, channel6, channel7, channel8, channel9, channel10]\n",
        "# # Define output path and filename prefix\n",
        "# output_path = os.path.join(root_dir,'output_folder')\n",
        "# print(\"output_path is {}\".format(output_path))\n",
        "\n",
        "# os.makedirs(output_path,exist_ok=True)\n",
        "# filename_prefix = 'output_prefix'\n",
        "# print(\"filename_prefix is {}\".format(filename_prefix))\n",
        "# output_shape = (224, 224)\n",
        "\n",
        "\n",
        "for npz_file_path in npy_n_channels_all[0:1]:\n",
        "    # default params: rgb_height=224, rgb_width=224\n",
        "    # Replace the pattern using regular expressions\n",
        "    print(\"npz_file_path is {}\".format(npz_file_path))\n",
        "    output_shape = (224, 224)\n",
        "    num_channels = 42\n",
        "    output_path = re.sub(r'NPY_.*_channels', 'JPG_RGB_{}_{}'.format(output_shape[0], output_shape[1]), npz_file_path)\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    output_path = os.path.dirname(output_path)\n",
        "    print(\"output_path is {}\".format(output_path))\n",
        "    patient = os.path.basename(npz_file_path).split(\".npz\")[0]\n",
        "    # Load the NPZ file\n",
        "    # Load the NPZ file\n",
        "    import numpy as np\n",
        "\n",
        "    # Load the NPZ file\n",
        "    loaded_data = np.load(npz_file_path)\n",
        "\n",
        "    # Check the keys present in the loaded NPZ file\n",
        "    print(\"Keys in the NPZ file:\", loaded_data.files)\n",
        "\n",
        "    # Now you have the extracted channel_data as a list of numpy arrays\n",
        "    channel_data = loaded_data['channel_data']\n",
        "\n",
        "    # Call the function to create and save RGB images\n",
        "    # create_rgb_images_from_channels(channel_data, output_shape, output_path, filename_prefix)\n",
        "   # create_rgb_images_from_channels(channel_data, output_shape, output_path, patient)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_RGB_224_224/labels/progressor\""
      ],
      "metadata": {
        "id": "GRQbAMRQgpMa"
      },
      "id": "GRQbAMRQgpMa",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.path.dirname(output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GbS5QWGQggqA",
        "outputId": "487d4238-91c6-4638-ca32-b3be3d7da1f7"
      },
      "id": "GbS5QWGQggqA",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_RGB_224_224/labels'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# root_folder = os.path.dirname(output_path)\n",
        "# root_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DJsOx1v0g5VL",
        "outputId": "2fb602f6-8c52-4235-d39a-ba7111782ef4"
      },
      "id": "DJsOx1v0g5VL",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_RGB_224_224/labels'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.path.join(os.path.dirname(root_folder),\"train_validation_test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U9mXzRZPg8UT",
        "outputId": "9de032a9-9bf3-450e-e5c8-8d2452477008"
      },
      "id": "U9mXzRZPg8UT",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_RGB_224_224/train_validation_test'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the train / validation and test folder for the created images:\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define the root folder containing the labels and images\n",
        "root_folder = os.path.dirname(output_path)\n",
        "print(\"root_folder is {}\".format(root_folder))\n",
        "output_folder = os.path.join(os.path.dirname(root_folder),\"train_validation_test\") # Define the output folder where train, validation, and test sets will be created\n",
        "print(\"output_folder is {}\".format(output_folder))\n",
        "\n",
        "# Define label names\n",
        "label_names = os.listdir(root_folder)\n",
        "\n",
        "# Define train, validation, and test ratios\n",
        "train_ratio = 0.7\n",
        "validation_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Create train, validation, and test folders\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    split_folder = os.path.join(output_folder, split)\n",
        "    os.makedirs(split_folder, exist_ok=True)\n",
        "    for label in label_names:\n",
        "        os.makedirs(os.path.join(split_folder, label), exist_ok=True)\n",
        "\n",
        "# Iterate through each label\n",
        "for label in label_names:\n",
        "    label_folder = os.path.join(root_folder, label)\n",
        "    image_files = [f for f in os.listdir(label_folder) if f.endswith('.png')]\n",
        "\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    num_samples = len(image_files)\n",
        "    num_train = int(num_samples * train_ratio)\n",
        "    num_validation = int(num_samples * validation_ratio)\n",
        "\n",
        "    train_images = image_files[:num_train]\n",
        "    validation_images = image_files[num_train:num_train + num_validation]\n",
        "    test_images = image_files[num_train + num_validation:]\n",
        "\n",
        "    for image in train_images:\n",
        "        src = os.path.join(label_folder, image)\n",
        "        dest = os.path.join(output_folder, 'train', label, image)\n",
        "        shutil.copy(src, dest)\n",
        "\n",
        "    for image in validation_images:\n",
        "        src = os.path.join(label_folder, image)\n",
        "        dest = os.path.join(output_folder, 'validation', label, image)\n",
        "        shutil.copy(src, dest)\n",
        "\n",
        "    for image in test_images:\n",
        "        src = os.path.join(label_folder, image)\n",
        "        dest = os.path.join(output_folder, 'test', label, image)\n",
        "        shutil.copy(src, dest)\n",
        "\n",
        "print(\"Data splitting and copying complete.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EFYv5Yo3gXst"
      },
      "id": "EFYv5Yo3gXst",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc40566-923f-4c70-90f2-5196f2cfd46b",
      "metadata": {
        "id": "4cc40566-923f-4c70-90f2-5196f2cfd46b",
        "outputId": "ababd7d6-836f-41bd-c723-1211da13b09f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43faa354-8729-48a1-9dc6-705243ba746d",
      "metadata": {
        "id": "43faa354-8729-48a1-9dc6-705243ba746d",
        "outputId": "3d7229b3-c1b7-4e67-db35-2fe434376d5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'image_file_path', 'labels'],\n",
              "    num_rows: 46\n",
              "})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b480bc6-22d0-46c9-9ef1-79801097c52c",
      "metadata": {
        "id": "5b480bc6-22d0-46c9-9ef1-79801097c52c",
        "outputId": "7cf66916-19b8-49d9-dbcd-c4bc4194d4fd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAD9UAAAADCAIAAAAYvSkgAAAAOklEQVR4nO3BMQEAAADCoPVPbQsvoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeBiOgAABl8s0ZQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=4053x3>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ex = train_dataset['image'][0]\n",
        "ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d0f362-1da1-401c-9666-ffe3ce233232",
      "metadata": {
        "id": "f2d0f362-1da1-401c-9666-ffe3ce233232"
      },
      "outputs": [],
      "source": [
        "# image = ex['image']\n",
        "# image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20515082-30b6-4126-a147-34a6e12bf8e6",
      "metadata": {
        "id": "20515082-30b6-4126-a147-34a6e12bf8e6",
        "outputId": "62f93120-b66a-4add-eb6c-58fb0b0afdaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = train_dataset['labels'][0]\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19821510-e8be-4a6c-a1b8-37a8d4db267b",
      "metadata": {
        "id": "19821510-e8be-4a6c-a1b8-37a8d4db267b",
        "outputId": "7025c851-7787-4012-e94f-b5c847a5a458"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'nonprogressor'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_string = class_label.int2str(labels)\n",
        "label_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3187afd-5931-4399-8877-ea736be96baa",
      "metadata": {
        "id": "f3187afd-5931-4399-8877-ea736be96baa"
      },
      "outputs": [],
      "source": [
        "# labels.ste(train_dataset['labels'][0])\n",
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "# Preprocessing function for images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f283d4-94c1-4277-890a-5b8dc72086d6",
      "metadata": {
        "id": "18f283d4-94c1-4277-890a-5b8dc72086d6"
      },
      "outputs": [],
      "source": [
        "# # Define a function to preprocess your custom dataset\n",
        "# def preprocess_custom_dataset(example):\n",
        "#     image = Image.open(example['image_file_path'])\n",
        "#     inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "#     return {\n",
        "#         'pixel_values': inputs['pixel_values'],\n",
        "#         'label': example['label']  # Replace with the actual way you load labels\n",
        "#     }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a158d4-6cc2-4acf-96ca-359799b8ded5",
      "metadata": {
        "id": "45a158d4-6cc2-4acf-96ca-359799b8ded5"
      },
      "outputs": [],
      "source": [
        "# from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "# from transformers import Trainer, TrainingArguments\n",
        "# from datasets import load_dataset\n",
        "# import torch\n",
        "# from PIL import Image\n",
        "\n",
        "# def preprocess_image(example):\n",
        "#     # Load the image using PIL\n",
        "#     image = Image.open(example['image_file_path'])\n",
        "#     # Use the feature extractor to preprocess the image\n",
        "#     inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "#     return inputs\n",
        "\n",
        "# # Load the feature extractor\n",
        "# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# preprocessed_train_and_validation_dataset = train_and_validation_dataset.map(preprocess_image)\n",
        "# preprocessed_test_dataset = test_dataset.map(preprocess_image)\n",
        "# # Load the ViT model\n",
        "# model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "# # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# # model.to(device)  # Move the model to the desired device\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # Set up training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     num_train_epochs=3,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     eval_steps=500,\n",
        "#     logging_dir='./logs',\n",
        "#     save_total_limit=3,\n",
        "# )\n",
        "\n",
        "# # Initialize the Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=preprocessed_train_and_validation_dataset,\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# trainer.train()\n",
        "\n",
        "# # Evaluate the model\n",
        "# eval_results = trainer.evaluate(preprocessed_test_dataset)\n",
        "# print(eval_results)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f330fbf2-ce88-4885-a2f6-d766426618ea",
      "metadata": {
        "id": "f330fbf2-ce88-4885-a2f6-d766426618ea",
        "outputId": "ac55bf7b-7363-4341-92bb-f1fb5c010ebc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eilaarich-landkof/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 51/51 [00:00<00:00, 90.95 examples/s] \n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/Users/eilaarich-landkof/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 51\u001b[0m\n\u001b[1;32m     44\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     45\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     46\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     47\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mpreprocessed_custom_dataset,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/trainer.py:2654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2657\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2678\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2679\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:804\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    802\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 804\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    813\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    815\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output[:, \u001b[38;5;241m0\u001b[39m, :])\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:583\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m expected_dtype:\n\u001b[1;32m    581\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[0;32m--> 583\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    588\u001b[0m     embedding_output,\n\u001b[1;32m    589\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:121\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    117\u001b[0m     pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    118\u001b[0m     bool_masked_pos: Optional[torch\u001b[38;5;241m.\u001b[39mBoolTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m     interpolate_pos_encoding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 121\u001b[0m     batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    122\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bool_masked_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ],
      "source": [
        "# from transformers import ViTForImageClassification, ViTFeatureExtractor, Trainer, TrainingArguments\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "\n",
        "# # Define a function to preprocess and extract patches from your custom dataset\n",
        "# def preprocess_custom_dataset(example):\n",
        "#     image = Image.open(example['image_file_path'])\n",
        "\n",
        "#     # Resize the image to the desired model input size (224x224)\n",
        "#     image = image.resize((224, 224))\n",
        "#     #!!TODO: change to patches if the resize works for the model, or think of something else\n",
        "#     # Use the feature extractor to preprocess the image and extract patches\n",
        "#     inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "#     return {\n",
        "#         'pixel_values': inputs['pixel_values'],\n",
        "#         'labels': example['labels']  # Replace with the actual way you load labels\n",
        "#     }\n",
        "\n",
        "# # Load the feature extractor\n",
        "# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# # Your existing custom Dataset object\n",
        "# # custom_dataset =train_and_validation_dataset\n",
        "\n",
        "# # Preprocess the custom dataset and extract patches\n",
        "# preprocessed_custom_dataset = train_and_validation_dataset.map(preprocess_custom_dataset)\n",
        "\n",
        "# # Load the ViT model\n",
        "# model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# # Set up training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     num_train_epochs=3,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     eval_steps=500,\n",
        "#     logging_dir='./logs',\n",
        "#     save_total_limit=3,\n",
        "# )\n",
        "\n",
        "# # Initialize the Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=preprocessed_custom_dataset,\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c521f10-d4bc-4b91-91c8-829f4401eb6e",
      "metadata": {
        "id": "0c521f10-d4bc-4b91-91c8-829f4401eb6e"
      },
      "outputs": [],
      "source": [
        "# preprocessed_custom_dataset['pixel_values']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68114641-6f63-49a5-8c1e-96cedc0640fe",
      "metadata": {
        "id": "68114641-6f63-49a5-8c1e-96cedc0640fe"
      },
      "outputs": [],
      "source": [
        "# # Initialize the Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=preprocessed_train_and_validation_dataset,\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# trainer.train()\n",
        "\n",
        "# # Evaluate the model\n",
        "# eval_results = trainer.evaluate(preprocessed_test_dataset)\n",
        "# print(eval_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9319ee67-49ac-4a25-9a64-7e446f17057d",
      "metadata": {
        "id": "9319ee67-49ac-4a25-9a64-7e446f17057d",
        "outputId": "52d5179f-7e58-42a0-9cbe-5e51bfb3fba2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eilaarich-landkof/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 30\u001b[0m\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     24\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     25\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     26\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mpreprocessed_train_and_validation_dataset,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(preprocessed_test_dataset)\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/trainer.py:2654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2654\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2657\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2678\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2679\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:804\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    802\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 804\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    813\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    815\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output[:, \u001b[38;5;241m0\u001b[39m, :])\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:583\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m expected_dtype:\n\u001b[1;32m    581\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[0;32m--> 583\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    588\u001b[0m     embedding_output,\n\u001b[1;32m    589\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:121\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    117\u001b[0m     pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    118\u001b[0m     bool_masked_pos: Optional[torch\u001b[38;5;241m.\u001b[39mBoolTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m     interpolate_pos_encoding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 121\u001b[0m     batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    122\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bool_masked_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ],
      "source": [
        "# # Set up training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     num_train_epochs=3,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     evaluation_strategy=\"epoch\",\n",
        "#     logging_dir='./logs',\n",
        "#     save_total_limit=3,\n",
        "# )\n",
        "\n",
        "# # Set up training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     num_train_epochs=3,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     eval_steps=500,\n",
        "#     logging_dir='./logs',\n",
        "#     save_total_limit=3,\n",
        "# )\n",
        "\n",
        "# # Initialize the Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=preprocessed_train_and_validation_dataset,\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# trainer.train()\n",
        "\n",
        "# # Evaluate the model\n",
        "# eval_results = trainer.evaluate(preprocessed_test_dataset)\n",
        "# print(eval_results)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}