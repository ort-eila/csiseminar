{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ort-eila/csiseminar/blob/main/project/step4_load_dataset_vit_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "OldUIqyzUm8d"
      },
      "outputs": [],
      "source": [
        "# # Cloud\n",
        "# !pip install transformers\n",
        "# !pip install --upgrade transformers\n",
        "# !pip install datasets\n",
        "# !pip install --upgrade datasets\n",
        "# !pip install transformers[torch]\n",
        "\n",
        "\n",
        "# # # for CPU:\n",
        "# # !pip install transformers[torch]\n",
        "# # # restart\n",
        "# MAC\n",
        "# # !pip install tensorboard\n",
        "# # !conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\n",
        "# # !conda install scikit-learn\n",
        "# # pip install transformers[torch]\n",
        "# # conda install transformers pytorch -c conda-forge -c pytorch\n",
        "# # pip install accelerate -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OWupabIrDMED",
        "outputId": "5d952c6e-dc05-4c8d-86fa-658766c0c112"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "rgb_images_path = \"\"\n",
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  !ls /Users"
      ],
      "metadata": {
        "id": "86hKbJVfVhLa"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-mLILCDD7DoP",
        "outputId": "15c6b501-6e2f-4c9f-f281-4c1ef23fccca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "YVN2-UK1DOvS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba43d12-c16e-4be6-8e09-43bb598bef70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "option 2\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if \"eilaarich-landkof-stanford\" in os.getcwd():\n",
        "  print(\"option 1\")\n",
        "  rgb_images_path = os.path.join(\"/Users/eilaarich-landkof-stanford/Downloads/train_validation_test_2023_08_14_20_18/\")\n",
        "elif \"/content\" == os.getcwd():\n",
        "  print(\"option 2\")\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/gdrive\")\n",
        "  rgb_images_path = \"/content/gdrive/MyDrive/train_validation_test_2023_08_14_20_18\"\n",
        "else:\n",
        "  print(\"option 3\")\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/gdrive\")\n",
        "  rgb_images_path = \"/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_VECT_TO_IMG_224_224/train_validation_test_2023_08_14_20_18\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(rgb_images_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcdMYv9cYij2",
        "outputId": "e2c58f46-6c9e-4ca2-9b86-f82716716dcb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['metadata.json',\n",
              " 'tensorboard',\n",
              " 'validation',\n",
              " 'train',\n",
              " 'test',\n",
              " 'model_output_dir',\n",
              " 'ds_with_pil_images']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdaEVkvJbAVG",
        "outputId": "cedcf1f7-1d03-47d1-c1d4-0da1365af1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets version: 2.14.4\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "print(\"datasets version:\", datasets.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "136y8YShdzqg"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "\n",
        "class_labels = ClassLabel(names=[\"nonprogressor\", \"progressor\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "xvBd6fc9a4Jd",
        "outputId": "3d1dee68-8b2b-41fa-957e-87a84c88337d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ds_with_pil_images_fld is /content/gdrive/MyDrive/train_validation_test_2023_08_14_20_18/ds_with_pil_images/train\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-3f5c6fcf27b0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mds_with_pil_images_fld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_images_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ds_with_pil_images'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Output metadata JSON file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ds_with_pil_images_fld is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_with_pil_images_fld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_with_pil_images_fld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Datasets saved to disk with PIL images. \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# will be used by step 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   1669\u001b[0m                     \u001b[0;34mf\"No such files: '{dataset_info_path}', nor '{dataset_state_json_path}' found. Expected to load a `Dataset` object, but got a `DatasetDict`. Please use either `datasets.load_from_disk` or `DatasetDict.load_from_disk` instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m                 )\n\u001b[0;32m-> 1671\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1672\u001b[0m                 \u001b[0;34mf\"No such files: '{dataset_info_path}', nor '{dataset_state_json_path}' found. Expected to load a `Dataset` object but provided path is not a `Dataset`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m             )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such files: '/content/gdrive/MyDrive/train_validation_test_2023_08_14_20_18/ds_with_pil_images/train/dataset_info.json', nor '/content/gdrive/MyDrive/train_validation_test_2023_08_14_20_18/ds_with_pil_images/train/state.json' found. Expected to load a `Dataset` object but provided path is not a `Dataset`."
          ]
        }
      ],
      "source": [
        "from datasets.io.abc import Dataset\n",
        "\n",
        "ds={}\n",
        "for val in [\"train\",\"validation\",\"test\"]:\n",
        "  ds_with_pil_images_fld = os.path.join(rgb_images_path,'ds_with_pil_images',val)   # Output metadata JSON file name\n",
        "  print(\"ds_with_pil_images_fld is {}\".format(ds_with_pil_images_fld))\n",
        "  ds[val] = Dataset.load_from_disk(ds_with_pil_images_fld)\n",
        "  print(\"Datasets saved to disk with PIL images. \",val)\n",
        "# will be used by step 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_Q_au_o7x10",
        "outputId": "5f6fecd9-a119-4f52-cbaf-ca883fd6394c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "\n",
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlCd0biJ8UhI",
        "outputId": "2aa31e9d-1c6c-4555-e888-b1de676c0e2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTFeatureExtractor {\n",
              "  \"do_normalize\": true,\n",
              "  \"do_rescale\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"image_mean\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"image_processor_type\": \"ViTFeatureExtractor\",\n",
              "  \"image_std\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"resample\": 2,\n",
              "  \"rescale_factor\": 0.00392156862745098,\n",
              "  \"size\": {\n",
              "    \"height\": 224,\n",
              "    \"width\": 224\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "feature_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9Tb_ZANCwch"
      },
      "outputs": [],
      "source": [
        "def augmentation_fn(image):\n",
        "  import torchvision.transforms as T\n",
        "  transform = T.Compose([\n",
        "    T.RandomHorizontalFlip(p=0.5),  # 50% chance of horizontal flip\n",
        "    T.RandomVerticalFlip(p=0.5),    # 50% chance of vertical flip\n",
        "    # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
        "    T.RandomRotation(degrees=10),   # Random rotation up to 10 degrees\n",
        "  ])\n",
        "\n",
        "  augmented_image = transform(image)\n",
        "  return augmented_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKmYgnTn7DoT",
        "outputId": "e4f2fbce-4cfc-49f7-954b-ce00c3d04e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image_path': '/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_VECT_TO_IMG_224_224/train_validation_test_2023_08_14_20_18/train/progressor/6103_vector_image_row_1023_col793.jpg', 'labels': 'progressor', 'pil_image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7D80D935B580>}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAACc0lEQVR4nO3b246CMBQFUIr+/x/bzkOThog4Ci0UXOvJIRHqdHN6EcNwCSGELW9PKbVuScVL/JRN/dq5z1NbKz3SWd396AY09DIWG2stO7tyQF9SzM5lPLoBVHDhYeHnKuhVvczoBYYLFZSuqaA1PVWsbkfeecO6rbUC2lC3vT6XI9thgw3xdE0FZb3pVKFR9RXQ01gahTv5+qpRWAX0ZLYvvA4M9IqZroCy0upb5as9BAG9rKM2uZbStq49AlrHiXYWe3uG5v0/yjZTKyGEbjfq57q9nVTQczgw60+X3jnKAtqX+/0+DENK6fF4HN2W10pep0ltd/+cZgzqXN29mzI9SCnV/X5/RXtqfbR151FBezQvTvlI6eP8IoSQC23DAlbvxnv6CB8S0E6V7ixZHCaxSCmFEKZ/zs/Q5xLt22QLaAUVo3C73YZhSCnFGEsoc0yfujbG+P5U3S7MvyKgFSwVsC0TvnEcy9tjjDm45Zyrw3eWB+9LOwW0lRW9Po5jjHE6V8uvc1jbxWjdvbQPAe3IfNTOMc3HD5lTHv4bgR7n0WRlhVSC+7SK/3caOj/hy+NvyucOT3y+v64K2q+lYX26rm+qrNiG2b7BbgT0TDaukL6V577bV2ab2rD/JeFzKiiL8hz3kPKZK/dgkcSbmeVRKyQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgF/wB2/LBWvOg2VqAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#test\n",
        "print(ds['train'][0])\n",
        "aug_example = augmentation_fn(ds['train'][0]['pil_image'])\n",
        "aug_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAslJflg9Bed",
        "outputId": "c02fd0d4-2c65-4e49-f667-8ce284127d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_name_prefix is norm_False_scl_False_resz_False_aug_T\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "do_normalize=False\n",
        "do_rescale=False\n",
        "do_resize=False\n",
        "model_name_prefix = \"norm_{}_scl_{}_resz_{}_aug_T\".format(do_normalize,do_rescale,do_resize)\n",
        "print(\"model_name_prefix is {}\".format(model_name_prefix))\n",
        "\n",
        "prepared_ds={}\n",
        "def transform(example_batch):\n",
        "  from PIL import Image\n",
        "    # Take a list of PIL images and turn them to pixel values\n",
        "  # print(\"example_batch is {}\".format(example_batch))\n",
        "  inputs = feature_extractor([Image.open(x) for x in example_batch['image_path']],\n",
        "                              do_normalize=do_normalize,\n",
        "                              do_rescale=do_rescale,\n",
        "                              do_resize=do_resize,\n",
        "                              augmentation_fn=augmentation_fn,\n",
        "                              return_tensors='pt')\n",
        "\n",
        "  # Extract PIL image objects from example_batch - need to be faster\n",
        "  # pil_images = [np.array(sample['pil_image']) for sample in example_batch]\n",
        "  # inputs = feature_extractor(pil_images,\n",
        "  #                            do_normalize=do_normalize,\n",
        "  #                            do_rescale=do_rescale,\n",
        "  #                            do_resize=do_resize,\n",
        "  #                            augmentation_fn=augmentation_fn,\n",
        "  #                            return_tensors='pt')\n",
        "\n",
        "\n",
        "    # Don't forget to include the labels!\n",
        "  # inputs['labels'] = [class_labels.str2int(sample['labels']) for sample in example_batch]\n",
        "  inputs['labels'] = example_batch['labels']\n",
        "  return inputs\n",
        "\n",
        "for val in [\"train\",\"validation\",\"test\"]:\n",
        "  prepared_ds[val] = ds[val].with_transform(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5fIwIo19IJH"
      },
      "outputs": [],
      "source": [
        "# prepared_ds['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KxxD_Qg9K2V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "        # 'pixel_values': torch.stack([x['pil_image'] for x in batch]),\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        # 'labels': torch.tensor([x['labels'] for x in batch])\n",
        "        'labels': torch.tensor([class_labels.str2int(x['labels']) for x in batch])\n",
        "    }\n",
        "\n",
        "    # class_labels.str2int\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRIhFyCB9QQS",
        "outputId": "791169a7-01b0-49fa-bdb1-0976b69dde87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-3044fb6e3895>:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDNaburKEA_U",
        "outputId": "be318c2d-c658-49cd-ef31-85020d0bbb43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nonprogressor', 'progressor']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "class_labels.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwAn4E3J9SIW",
        "outputId": "4fbcc282-a7e1-4461-9577-7a6da0762421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n",
        "labels = class_labels.names #ds['train'].features['labels'].names\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g1upvHM1Od_s",
        "outputId": "865bbf6a-599d-4768-8676-f02a93f872e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023_08_15_02_08_15'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Generate a date-time string\n",
        "from datetime import datetime\n",
        "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "current_datetime = current_datetime.replace(\"-\",\"_\")\n",
        "current_datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "0CHxNIZ1TgDS",
        "outputId": "5ccb59c9-cd9f-49da-ac74-89ff159ab569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_name is _lr_0_005_epochs_50_batch_size_32\n",
            "model_output_dir is /content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_VECT_TO_IMG_224_224/train_validation_test_2023_08_14_20_18/model_output_dir/norm_False_scl_False_resz_False_aug_T_lr_0_005_epochs_50_batch_size_32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_VECT_TO_IMG_224_224/train_validation_test_2023_08_14_20_18/tensorboard/2023_08_15_02_08_15_lr_0_005_epochs_50_batch_size_32'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
        "\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# # Create callbacks\n",
        "# train_dataclass_weights=class_weights,\n",
        "#\n",
        "\n",
        "lr = 5e-3\n",
        "num_train_epochs = 50\n",
        "batch_size = 16\n",
        "# early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=5)\n",
        "# class_weights = compute_class_weight('balanced', classes=[0, 1], y=prepared_ds[\"train\"][\"labels\"])\n",
        "model_name = \"_lr_{}_epochs_{}_batch_size_{}\".format(str(lr).replace(\".\",\"_\"),num_train_epochs,batch_size)\n",
        "print(\"model_name is {}\".format(model_name))\n",
        "# evaluation_strategy = \"epoch\"\n",
        "# optimizers\n",
        "# callbacks\n",
        "model_output_dir = os.path.join(rgb_images_path, \"model_output_dir\",model_name_prefix+model_name)\n",
        "print(\"model_output_dir is {}\".format(model_output_dir))\n",
        "\n",
        "os.makedirs(model_output_dir,exist_ok=True)\n",
        "model_output_dir\n",
        "\n",
        "tensorboard_log = os.path.join(rgb_images_path, \"tensorboard\",current_datetime+model_name)\n",
        "os.makedirs(tensorboard_log,exist_ok=True)\n",
        "tensorboard_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yxWtYLBdWzMT",
        "outputId": "81b046ab-4e5d-48df-ba55-f8cf59a05537"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Seminar/project/immune_310_project/dataset/JPG_VECT_TO_IMG_224_224/train_validation_test_2023_08_14_20_18/tensorboard/2023_08_15_02_08_15_lr_0_005_epochs_50_batch_size_32'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "tensorboard_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmmWoDbLEaGI"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=model_output_dir,\n",
        "  per_device_train_batch_size=batch_size,\n",
        "  evaluation_strategy = \"epoch\",\n",
        "  logging_strategy = \"epoch\",\n",
        "  save_strategy = \"epoch\",\n",
        "  num_train_epochs=num_train_epochs,\n",
        "  save_steps=100,\n",
        "  eval_steps = 50,\n",
        "  logging_steps=10,\n",
        "  learning_rate=lr,\n",
        "  save_total_limit=5,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        "  # fp16=False,  # Disable mixed precision training. The True had error issues\n",
        "  fp16_opt_level=\"O2\",  # Specify optimization level\n",
        "  logging_dir=tensorboard_log, # Set the logging directory\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKI-oRZHz_hD",
        "outputId": "791f5c5c-78d0-4d57-a141-7ea1e9839f61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': Dataset({\n",
              "     features: ['image_path', 'labels', 'pil_image'],\n",
              "     num_rows: 30519\n",
              " }),\n",
              " 'validation': Dataset({\n",
              "     features: ['image_path', 'labels', 'pil_image'],\n",
              "     num_rows: 6539\n",
              " }),\n",
              " 'test': Dataset({\n",
              "     features: ['image_path', 'labels', 'pil_image'],\n",
              "     num_rows: 6542\n",
              " })}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "prepared_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gaGlHFCEfoP"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=prepared_ds[\"train\"],\n",
        "    eval_dataset=prepared_ds[\"validation\"],\n",
        "    # callbacks = [early_stopping_callback],\n",
        "    tokenizer=feature_extractor,\n",
        "    # train_dataclass_weights=class_weights,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2K57vfFH0zc",
        "outputId": "53e3f2fd-a360-4cff-8e25-7cbc80050722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_results = trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzP1pDgSOs59"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfFTAiUQu4ck"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNrJHsTEy4vh"
      },
      "outputs": [],
      "source": [
        "prepared_ds['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3stKYrWBH4dW"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate(prepared_ds['test'])\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVsRPOwqH8tm"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"finetuned_from\": model.config._name_or_path,\n",
        "    \"tasks\": \"image-classification\",\n",
        "    \"dataset\": 'dcis-eila',\n",
        "    \"tags\": ['image-classification'],\n",
        "}\n",
        "\n",
        "if training_args.push_to_hub:\n",
        "    trainer.push_to_hub('üçª cheers', **kwargs)\n",
        "else:\n",
        "    trainer.create_model_card(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfsyFURk18Fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkadroVuOyWV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPW3s-6JOyjA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxP2o14hOyo4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}